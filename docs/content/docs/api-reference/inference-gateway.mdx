---
title: "Inference Gateway"
description: "OpenAI-compatible inference endpoints"
openapi: "openapi-inference.json"
---

The Inference Gateway provides high-performance, streaming-compatible endpoints for LLM interaction. It is fully compatible with the OpenAI API specification.

## Base URL
`http://localhost:8001/v1`

## Endpoints

### Chat Completions


#### `model`
<div className="flex gap-2 text-sm text-muted-foreground mb-2">
  <span className="px-2 py-0.5 rounded-md bg-secondary border">Body</span>
  <span className="px-2 py-0.5 rounded-md bg-secondary border">string</span>
  <span className="px-2 py-0.5 rounded-md bg-primary/10 text-primary border border-primary/20">Required</span>
</div>

The name of the deployment to use (e.g., `llama-3-8b`).



#### `messages`
<div className="flex gap-2 text-sm text-muted-foreground mb-2">
  <span className="px-2 py-0.5 rounded-md bg-secondary border">Body</span>
  <span className="px-2 py-0.5 rounded-md bg-secondary border">array</span>
  <span className="px-2 py-0.5 rounded-md bg-primary/10 text-primary border border-primary/20">Required</span>
</div>

A list of messages comprising the conversation so far.



#### `stream`
<div className="flex gap-2 text-sm text-muted-foreground mb-2">
  <span className="px-2 py-0.5 rounded-md bg-secondary border">Body</span>
  <span className="px-2 py-0.5 rounded-md bg-secondary border">boolean</span>
  
</div>

If `true`, partial message deltas will be sent via Server-Sent Events.


```bash
curl http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $API_KEY" \
  -d '{
    "model": "llama-3-8b",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```
